{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68cd73a9-ac97-44ca-91b4-d7b831d899ad",
   "metadata": {},
   "source": [
    "# Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79326383-cb90-49c6-989a-97ac9c1dda01",
   "metadata": {},
   "source": [
    "* It involves *stratifying* or *segmenting* the predictor space into a number of simple regions - to make a prediction for a given observation, we typically use the *mean* or the *mode* response value for the training observations in the region to which it belongs - since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as *decision tree methods*.\n",
    "* These approaches are simple and useful for interpretation, but they typically are not competitive as like discussed in [6th](http://localhost:8888/lab/tree/06_Linear%20Model%20Selection%20and%20Regularization.ipynb) and [7th](http://localhost:8888/lab/tree/07_Moving%20Beyond%20Linearity.ipynb) chapters!\n",
    "* Approaches like *bagging*, *random forests*, *boosting*, and *Bayesian additive regression trees* involves producing multiple trees which are then combined to yield a single consensus prediction.\n",
    "* ***P.S.*** combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.\n",
    "* Decision trees can be applied to both regression and classification problems.\n",
    "* Decision trees are typically drawn *upside down* meaning leaves are at the bottom of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7de658-12ca-4292-ad94-da296dffba3b",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "1.  Divide the predictor space i.e.,the set of possible values for $X_1,X_2,\\ldots,X_p$ into *J* no.of. non overlapping regions, $R_1,R_2,\\ldots,R_J$ - as per the *tree* analogy, these regions are known as *terminal nodes* or *leaves* of the tree.\n",
    "   - In theory, the regions could have any shape, but we choose to divide the predictor space into high dimensional rectangles, or boxes; just for simplicity and for ease of interpretation of the resulting predictive model.\n",
    "   - Goal is to find the regions $R_1,R_2,\\ldots,R_J$ that minimizes RSS, which is given as $RSS=\\sum_{j=1}^{J} \\sum_{i\\in R_j} \\left(y_i-\\hat{y}_{{R}_{j}} \\right)^2$; while $\\hat{y}_{{R}_{j}}$ is the mean response for the training observations within the $j^th$ box.\n",
    "   - But, it is computationally infeasible to consider every possible partition of the feature space into J boxes, so we use a *top-down, greedy* approach that is known as *recursive binary splitting*.\n",
    "   - It is known as *top-down* because it begins at the top of the tree , where all observations belong to a single region, then successively splits the predictor space; each split is indicated via two new branches further down on the tree.\n",
    "   - It is known as *greedy* because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "   - To perform recursive binary splitting, select the predictor $X_j$ and *cutpoint* *s* such that splitting the predictor space into the regions $\\{X|X_j <s\\}$ and $\\{X|X_j \\geq s\\}$ (the notation simply means the region of predictor space in which $X_j$ takes value less than/ greater than or equal to *s*) leads to the greatest possible reduction in RSS.\n",
    "   - That is we define the pair of half-planes $R_1(j,s)=\\{X|X_j < s \\}$ and $R_2(j,s)=\\{X|X_j \\geq s \\}$ and we seek the value of j and s that minimize the equation ${\\sum_{i:x_i \\in R_1 (j,s)}\\left(y_i-\\hat{y}_{{R}_{1}} \\right)^2}+{\\sum_{i:x_i \\in R_2 (j,s)}\\left(y_i-\\hat{y}_{{R}_{2}} \\right)^2}$\n",
    "   - We repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.\n",
    "   - But this time, instead of splitting the entire predictor space, we split one of the two previously identified regions - so we have 3 regions.\n",
    "   - Again, we look to split one of these three regions further, so as to minimize the RSS.\n",
    "   - The process continues until a stopping criterion is reached - we may continue until no region contains more than five observations.\n",
    "2. For every observation that falls into the region $R_j$ , we make the same prediction, meaning the mean of the response values for the training observations in $R_j$.\n",
    "  - Once the regions $R_1,R_2,\\ldots,R_J$ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\n",
    "\n",
    "### Tree Pruning\n",
    "* The above approach may perform well on the training set, but is likely to overfit the data, leading to poor test set performance because the resulting tree might be too complex.\n",
    "* A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.\n",
    "* A better strategy is to grow a very large tree $T_0$ and then *prune* it back in order to obtain a *subtree* - our goal is to select a subtree that leads to the lowest test error rate - Given a subtree, we can estimate its test error using cross-validation or the validation set approach but it is cumbersome since there is an extremely large number of possible subtrees - So, we need a way to select a small set of subtrees for consideration.\n",
    "* *Cost complexity pruning* a.k.a *weakest link pruning* approach considers a sequence of trees indexed by a non- negative tuning parameter $\\alpha$ such that for each value of $\\alpha$, there corresponds a subtree $T\\subset T_0$ so that this term $\\sum_{m=1}^{|T|}\\sum_{i: x_i \\in R_m}\\left(y_i - \\hat{y}_{R_m}\\right)^2 + \\alpha |T|$ is as small as possible, <br>where,<br>|T| is no. of terminal nodes of the tree *T*,<br>$R_m$ is the rectangle (subset of predictor space) corresponding to the $m^{th}$ terminal node <br> $\\hat{y}_{R_m}$ is the mean of the training observations in $R_m$<br> $\\alpha$ is the tuning parameter controls a trade-off between the subtree’s complexity and its fit to the training data.\n",
    "* It is comparable with *Lasso Regression* approach, where such a similar formulation was used in order to control the complexity of a linear model.\n",
    "* As $\\alpha$ increases from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of $\\alpha$ is easy.\n",
    "* The $\\alpha$ is selected using a validation set or using cross-validation approaches.\n",
    "* We then return to the full data set and obtain the subtree corresponding to $\\alpha$.\n",
    "### Algorithm for Building a Regression Tree\n",
    "1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "2. Apply *Cost complexity pruning* to the larger tree, to obtain a sequence of best subtrees, as a function of $\\alpha$.\n",
    "3. Use K-fold cross-validation to choose $\\alpha$ i.e., divide the training observations into K folds - repeat steps 1 and 2 on all kth folds, evaluate the mean squared prediction error on the data in the left-out kth fold, as a function of $\\alpha$ - average the results for each value of $\\alpha$ and pick $\\alpha$ to minimize the error. \n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970a917-9e14-443b-ab12-eb7fec328fd3",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "* A *classification tree* is used to predict a qualitative response.\n",
    "* We predict that each observation belongs to *the most commonly occurring class* of training observations in the region to which it belongs.\n",
    "* For interpreting the results, we are interested not only in the *class prediction* to a particular terminal node region but also in the *class proportions* that the training observations that fall into that region.\n",
    "* Like regression trees, we use recursive binary splitting to grow a classification tree, but instead of RSS to be used be used as a criterion for making the binary splits, we use *classification error rate* which is the fraction of the training observations in that region that do not belong to the most common class and it is given by $E = 1 - \\max_k(\\hat{p}_{mk})$, where, $\\hat{p}_{mk}$ is the proportion of training observations in the mth region that are from the kth class - in addition to *classification error rate*, we define *Gini index* and *Entropy*.\n",
    "* *Gini index* is a measure of total variance across the K classes, and is given by the eqn, $\\text{Gini index}, G = \\sum_{k=1}^{K}\\hat{p}_{mk}\\left(1-\\hat{p}_{mk}\\right)$ - Gini index takes on a small value if all $\\hat{p}_{mk}$ are close to 0 or 1 - so it is also known as measure of *node purity*, thus small Gini index means a node contains predominantly observations from a single class.\n",
    "* *Entropy* is given by the eqn, $\\text{Entropy}, D = -\\sum_{k=1}^{K}\\hat{p}_{mk}log(\\hat{p}_{mk})$ - Entropy will take the values near 0, if all the $\\hat{p}_{mk}$'s are near 0 or near 1 (i.e. $0 \\leq \\hat{p}_{mk} \\leq 1 $ follows that $0 \\leq - \\hat{p}_{mk}log(\\hat{p}_{mk})$) - it will take small value if the $m^{th}$ node is pure.\n",
    "* In a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split (these two factors are more sensitive to *node purity* than *classification error rate*).\n",
    "* We use *classification error rate* in *prunning* if prediction accuracy of the final pruned tree is the goal.\n",
    "### Trees Versus Linear Models\n",
    "* If the relationship between the features and the response is well approximated by a linear model, then approaches like linear regressions will outperform regression trees, while if there is a highly nonlinear and complex relationship between the features and the response then decision trees may outperform classical approaches.\n",
    "* Also note that apart from test error metrics in linear and tree approaches, for the sake of interpretability and visualization, tree approaches are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370377ef-c004-4fb6-9bff-29e234cb39e0",
   "metadata": {},
   "source": [
    "### Pros of Trees\n",
    "✅ Very easy to explain to people than linear regression.<br>\n",
    "✅ Some people believe that decision trees more closely mirror human decision-making than linear regression or classifications.<br>\n",
    "✅ Can be displayed graphically so that it is easily intrepretable by even non-experts(for a small trees).<br>\n",
    "✅ Can easily handle qualitative predictors without the need to create dummy variables.\n",
    "### Cons of Trees\n",
    "❌ Predictive Accuracy for trees do not have the same level as that of classical approaches. <br>\n",
    "❌ A small change in the data can cause a large change in the final estimated tree (*non-robust*).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25625d5d-3ef5-4ee8-8166-916c89ff08ea",
   "metadata": {},
   "source": [
    "## Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees\n",
    "* An *ensemble method* is an approach that combines many simple “building block” models in order to obtain a single and potentially very powerful model.\n",
    "* These simple “building block” models are sometimes known as *weak learners*, since they may lead to mediocre predictions on their own.\n",
    "* *Bagging*, *Random Forests*, *Boosting*, and *Bayesian Additive Regression Trees* are some ensemble methods for which the simple building block is a regression or a classification tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58a1b8-e044-4d53-a7e3-40ea03699ac5",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "* Earlier studied *bootstrap* technique, a powerful idea is used in many situations in which it is hard or even impossible to directly compute the standard deviation of a quantity of interest.\n",
    "* *Bootstrap aggregation* or *bagging*, is a general-purpose procedure for reducing the variance of a statistical learning method, particularly useful and frequently used in the context of decision trees.\n",
    "* We had earlier learnt that *averaging a set of observations reduces variance*, hence we would take many training sets from the population, build a separate prediction model using each training set and average the resulting predictions, i.e we could calculate $\\hat{f}^1(x),\\hat{f}^2(x),\\ldots,\\hat{f}^B(x)$ for $B$ training sets and calculate the average by $\\hat{f}_{avg}(x)=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^b(x)$. But in reality, we generally do not have access to multiple training sets and we have to bootstrap,by taking repeated samples from the (single) training data set and we generate $B$ different bootstrapped training data sets. We then train our method on the bth bootstrapped training set in order to get $\\hat{f}^{*b}(x)$ and finally average all the predictions $\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{*b}(x)$, and it is called *bagging*.\n",
    "* This can improve predictions for many regression methods, particularly useful for decision trees.\n",
    "* The number of trees B is not a critical parameter with bagging; using a very large value of $B$ will not lead to overfitting, so in practice, we use a value of $B$ sufficiently large, so that the error will be settled down.\n",
    "### Bagging in Quantitative responses (Regression Trees)\n",
    "* We simply construct $B$ regression trees using $B$ bootstrapped training sets, and average the resulting predictions.\n",
    "* These trees are grown deep, and are not pruned; hence individual trees has high variance, but low bias; and so we average the $B$ trees to reduce the variance.\n",
    "* Bagging gives impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.\n",
    "### Bagging in Qualitative responses (Classification)\n",
    "* For a given test observation, we can record the class predicted by each of the $B$ trees, and take a *majority vote*: the overall prediction is the most commonly occurring class among the $B$ predictions.\n",
    "### Out-of-Bag Error Estimation\n",
    "* We know that, the trees are repeatedly fit to bootstrapped subsets of the observations, it can be known that, on average, each bagged tree makes use of around two-thirds of the observations and the remaining one-third of the observations not used to fit a given bagged tree are referred to as the *out-of-bag* (OOB) observations.\n",
    "* In the OOB observations, predict the response for $i^{th}$ observation using each of the trees, which will yield around $B/3$ predictions for the $i^{th}$ observation.\n",
    "* To obtain a single prediction for the $i^{th}$ observation, we take average, if it is regression or we take majority vote, in the case of classifications.\n",
    "* An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (in the case of regression approach) or classification error (for a classification problem) can be computed.\n",
    "* This resulting OOB error is a valid estimate of the test error for the bagged model.\n",
    "* The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\n",
    "### Variable Importance Measures\n",
    "* Inspite of the bagging approach, being able to provide improved accuracy over prediction using a single tree, one may admit that it can be difficult to interpret the resulting model which was against the advantages of Decision Trees!\n",
    "* When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure.\n",
    "* Bagging improves prediction accuracy at the expense of interpretability.\n",
    "* But one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).\n",
    "* In bagging regression trees, we can record the total amount that the RSS ( $RSS=\\sum_{j=1}^{J} \\sum_{i\\in R_j} \\left(y_i-\\hat{y}_{{R}_{j}} \\right)^2$;) is decreased due to splits over a given predictor, averaged over all $B$ trees; while in the case of bagging classification, trees, we can add up the total amount that the Gini index ($\\text{Gini index}, G = \\sum_{k=1}^{K}\\hat{p}_{mk}\\left(1-\\hat{p}_{mk}\\right)$) is decreased by splits over a given predictor, averaged over all $B$ trees and the variables with the largest mean decrease in Gini index.\n",
    "* ***A large value indicatesan important predictor***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed25d9-67f6-4cfc-9ea7-6752560a9e2d",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "* *Random Forests* approach is an improvement over bagged trees by way of a small tweak that decorrelates the trees.\n",
    "* While in bagging, we build a number of decision trees on bootstrapped training samples, in random forests approach, when building these decision trees, each time a split in a tree is considered, *a random sample of $m$ predictors* is chosen as split candidates from the full set of $p$ predictors and given that the split is allowed to use only one of those $m$ predictors! ***P.S.*** *A fresh sample of $m$ predictors is taken at each split*.\n",
    "* Typically we choose $m \\approx \\sqrt{p}$.\n",
    "* We can say that in building a random forest, at each split in the tree, the algorithm is *not even allowed to consider* a majority of the available predictors!\n",
    "* Suppose that there is less number of (say,one) very strong predictor in the data set, along with a good number of other moderately strong predictors: then by bagging, most or all of the trees will use this strong predictor in the top split, thus all of the bagged trees will look quite similar! Hence the predictions from the bagged trees will be highly correlated; But averaging many highly correlated quantities does not lead to as large of a reduction in variance; thus bagging will not lead to a substantial reduction in variance over a single tree in this setting.\n",
    "* Random forests overcome this problem by forcing each split to consider only a subset of the predictors - on average $(p − m)/p$ of the splits will not even consider the strong predictor, and so other predictors will have more of a chance - known as *decorrelating* the trees making the average of the resulting trees less variable and hence more reliable.\n",
    "* The main difference between bagging and random forests is the choice of predictor subset size $m$. If m=p, then this amounts simply to 'bagging'!\n",
    "* Using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors.\n",
    "* As with bagging, random forests will not overfit if we increase $B$, so in practice we use a value of $B$ sufficiently large for the error rate to have settled down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd1131-5ffe-457a-8c6d-8181794e31ed",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "* *Boosting* is a general approach that can be applied to many statistical learning methods for regression or classification.\n",
    "* We knew that in bagging, each tree is built on a bootstrap data set, independent of the other trees; but in boosting, the trees are grown *sequentially*: each tree is grown using information from previously grown trees.\n",
    "* Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "* The boosting approach *learns slowly*.\n",
    "* Please refer to [Notes](boosting_notes.md) for further algorithmic explainations.\n",
    "### Some notes on the parameters in Boosting\n",
    "#### Number of trees (B)\n",
    "* Unlike bagging and random forests, boosting can overfit if B is too large and verfitting tends to occur slowly if at all.\n",
    "* We use cross-validation to select $B$.\n",
    "#### Shrinkage parameter ($\\lambda$)\n",
    "* A small positive number that controls the rate at which boosting learns (may be seen as learning rate?)\n",
    "* Typical values are 0.01 or 0.001, although the right choice can depend on the problem.\n",
    "* Very small $\\lambda$ can require using a very large value of B in order to achieve good performance.\n",
    "#### Number of splits OR depth of the tree (d)\n",
    "* It controls the complexity of the boosted ensemble.\n",
    "* Often $d=1$ is used and it works well, where each tree is a *stump*, consisting of a single split - boosted ensemble is fitting an additive model - since each term involves only a single variable.\n",
    "* It is the *interaction depth*, and controls interaction the interaction order of the boosted model - since *d* steps involves *d* variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71956c-df2c-4ee1-b67c-ea5275ddb390",
   "metadata": {},
   "source": [
    "## Bayesian Additive Regression Trees (BART)\n",
    "* We recall that bagging and random forests make predictions from an average of regression trees - each tree is built separately from the others; while in boosting uses a weighted sum of trees, each of which is constructed by fitting a tree to the residual of the current fit.\n",
    "* In BART, combination of approaches is used: each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting - main novelty is in way in which new trees are generated.\n",
    "* ***Notations used in BART algotithm***: $K \\rightarrow$ number of regression trees.<br>$B \\rightarrow$ number of iterations for which the BART algorithm will be run.<br>$\\hat{f}_{k}^{b} \\rightarrow$ prediction at $x$ for the $k^{th}$ regression tree used in the $b^{th} iteration.$<br> At the end of each iteration, the $K$ trees from that iteration will be summed i.e. $\\hat{f}^{b} = \\sum_{k=1}^{K} \\hat{f}_{k}^{b}(x)$ for $b=1,\\ldots,B$.\n",
    "* So at the first iteration of BART, all trees are initialized to have a single root node with $\\hat{f}_{k}^{1}(x)=\\frac{1}{nK}\\sum_{i=1}^{n}y_i$, the mean of the response values divided by the total number of trees $\\hat{f}^{1}(x)=\\sum_{k=1}^{K} \\hat{f}_{k}^{1}(x) = \\frac{1}{n}\\sum_{i=1}^{n}y_i$.\n",
    "* BART updates each of the K trees, one at a time, in subsequent iterations.\n",
    "* In the $b^{th}$ iteration, to update $k^{th}$tree, subtract from each response value the predictions from all but the $k^{th}$ tree, which yields *partial residual* given as $\\text{partial residual}, r_i= {y_i}- {\\sum_{k'<k}\\hat{f}_{k'}^{b}(x_i)}-{\\sum_{k'>k}\\hat{f}_{k'}^{b-1}(x_i)}$ for the ith observation, where $i=1,\\ldots,n$\n",
    "* Instead of fitting a fresh tree to this partial residual, BART randomly chooses a *perturbation* to the tree from the previous iteration $\\left( f_{k}^{b-1} \\right)$ from a set of possible perturbations, favoring ones that improve the fit to the partial residual.\n",
    "* The perturbation has 2 components:\n",
    "  1. We may change the structure of the tree by adding or pruning branches.\n",
    "  2. We may change the prediction in each terminal node of the tree.\n",
    "* The output of BART is a collection of prediction models $\\hat{f}^{b}(x)=\\sum_{k=1}^{K} \\hat{f}_{k}^{b}(x)$ for $b=1,2,\\ldots,B$.\n",
    "* We typically throw away the first few of these prediction models, since models obtained in the earlier iterations — known as the *burn-in period*— tend not to provide very good results and let, $L \\rightarrow$ number of burn-in iterations, then to obtain a single prediction, we simply take the average after the burn-in iterations, given as $\\hat{f}(x)=\\frac{1}{B-L} \\sum_{b=L+1}^{B} \\hat{f}^{b}(x)$.\n",
    "* It is also possible to compute quantities other than the average: for instance, the percentiles of $\\hat{f}^{L+1}(x), \\ldots,\\hat{f}^{B}(x)$ provide a measure of uncertainty in the final prediction.\n",
    "* From the $3^{rd}$ step of the algorithm, we do not fit a fresh tree to the current partial residual: instead, we try to improve the fit to the current partial residual by slightly modifying the tree obtained in the previous iteration - this guards against overfitting since it limits how “hard” we fit the data in each iteration.\n",
    "* The individual trees are typically quite small - we limit the tree size in order to avoid overfitting the data (which may occur in large trees).\n",
    "* Each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from a *posterior distribution* (*Bayesian* approach).\n",
    "* When we apply BART, we must select the number of trees $K$, the number of iterations $B$, and the number of burn-in iterations $L$- we typically choose large values for $B$ and $K$, and a moderate value for $L$.\n",
    "* Algorithm can be expalined in [Notes](bart_notes.md) or [here](bart_notes_1.md) and can be viewed as a *Markov chain Monte Carlo* algorithm for fitting the BART model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81e646-bf88-42d9-87a0-623bf696e0e3",
   "metadata": {},
   "source": [
    "# Summary of Tree Ensemble Methods\n",
    "\n",
    "| **Method**      | **Description**                                                                                                                                                                                                                                                                           |\n",
    "|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Bagging**     | - Trees are grown independently on random samples of the observations.<br>- Trees tend to be quite similar to each other.<br>- Can get caught in local optima and fail to thoroughly explore the model space.                                                                             |\n",
    "| **Random Forests** | - Trees are also grown independently on random samples of the observations.<br>- Each split uses a random subset of the features, decorrelating the trees.<br>- Leads to a more thorough exploration of model space relative to bagging.                                                |\n",
    "| **Boosting**    | - Only the original data is used (no resampling).<br>- Trees are grown successively in a “slow” learning approach.<br>- Each new tree is fit to the residual signal left over from earlier trees, and is shrunk before use.                                                                |\n",
    "| **BART**        | - Uses only the original data.<br>- Trees are grown successively.<br>- Each tree is perturbed to avoid local minima and achieve more thorough exploration of the model space.                                                                                                             |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
